{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Generation\n",
    "In this notebook we generate a dictionary for [PySpellChecker](https://pyspellchecker.readthedocs.io/en/latest/index.html) that we will use when preprocessing PDF files. We do so by doing the following:\n",
    "1. Downloading a frequency list from the Real Academia Española [Reference Corpus (Corpus de Referencia del Español Actual (CREA))](http://corpus.rae.es/lfrecuencias.html).\n",
    "2. Parsing the RAE frequency list into a JSON file that PySpellChecker can read.\n",
    "3. Creating a dictionary by using the documents we downloaded as HTML.\n",
    "4. Merging both dictionaries into a PySpellChecker custom dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "from collections import Counter \n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading CREA frequency list.\n",
    "\n",
    "We begin by downloading the CREA list and saving it to a temporary directory. The list comes in a zip file which we will have to extract later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('temp'):\n",
    "    os.mkdir('temp')\n",
    "\n",
    "r = requests.get('http://corpus.rae.es/frec/CREA_total.zip')\n",
    "with open('temp/CREA_Dictionary.zip', 'wb') as fp:\n",
    "    fp.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the zip file to obtain a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CREA_Dictionary.zip', 'CREA_total.TXT']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('temp/CREA_Dictionary.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('temp/')\n",
    "    \n",
    "os.listdir('temp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing the CREA list into a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have `CREA_total.TXT`, which contains frequencies for words in Spanish. We will parse this list into a dictionary which holds the frequency counts. We do so by reading it as a TSV file, since this file has this structure. We will hence use the `csv` module in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "raeCounts = {}\n",
    "with open('temp/CREA_total.TXT', encoding = \"ISO-8859-1\") as fp:\n",
    "    raw = csv.reader(fp, delimiter = '\\t')\n",
    "    next(raw)\n",
    "    for row in raw:\n",
    "        word = re.sub('\\W+', '', row[1])\n",
    "        raeCounts[word] = int(row[2].replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raeCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save this frequency list dictionary as a JSON file in case we need to reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp/dictRae.json', 'w') as fp:\n",
    "    json.dump(raeCounts, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we pass this JSON file to a PySpellChecker object that we will later save as our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customDictionary = SpellChecker(language = None)\n",
    "customDictionary.word_frequency.load_dictionary('temp/dictRae.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a dictionary from HTML documents\n",
    "Now we load the documents we got and parsed from HTML files. We have stored this into the `data/parsedHTML` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.loaders import loadCorpusList, loadCorpusDict, saveCorpus\n",
    "\n",
    "corpusPath = '../data/parsedHTML'\n",
    "\n",
    "corpusList = loadCorpusList(corpusPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we count all the words in these files. We will only count words not recognized yet by our dictionary. We will not add them yet though, since we will only count those that are the most frequent. This will help both add only relevant words but also filter out some artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "philWordCount = Counter()\n",
    "for doc in corpusList:\n",
    "    docWords = re.findall('\\w+', doc.cleanText)\n",
    "    for word in [word for word in docWords if word not in customDictionary]:\n",
    "        philWordCount[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aristotle', 498),\n",
       " ('responsibility', 203),\n",
       " ('καὶ', 146),\n",
       " ('Korsgaard', 140),\n",
       " ('Duica', 133),\n",
       " ('rhetoric', 132),\n",
       " ('Phaedo', 121),\n",
       " ('possibility', 116),\n",
       " ('Eriúgena', 112),\n",
       " ('Brandom', 111),\n",
       " ('philosopher', 106),\n",
       " ('Rancière', 104),\n",
       " ('Zahavi', 104),\n",
       " ('Socratic', 97),\n",
       " ('Hrsg', 94),\n",
       " ('τὸ', 94),\n",
       " ('KRV', 90),\n",
       " ('Callicles', 90),\n",
       " ('necessity', 89),\n",
       " ('Pereboom', 85)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "philWordCount.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice many of these words are words in English. We can also remove them by importing the default PySpellChecker English dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDictionary = SpellChecker(language = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "philWordCount = Counter()\n",
    "for doc in corpusList:\n",
    "    docWords = re.findall('\\w+', doc.cleanText)\n",
    "    for word in [word for word in docWords if word not in customDictionary and word not in englishDictionary]:\n",
    "        philWordCount[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('καὶ', 146),\n",
       " ('Korsgaard', 140),\n",
       " ('Duica', 133),\n",
       " ('Phaedo', 121),\n",
       " ('Eriúgena', 112),\n",
       " ('Brandom', 111),\n",
       " ('Rancière', 104),\n",
       " ('Zahavi', 104),\n",
       " ('Hrsg', 94),\n",
       " ('τὸ', 94),\n",
       " ('KRV', 90),\n",
       " ('Callicles', 90),\n",
       " ('Pereboom', 85),\n",
       " ('Badiou', 83),\n",
       " ('autoadscriptivo', 81),\n",
       " ('KrV', 76),\n",
       " ('princípios', 75),\n",
       " ('Gesinnung', 72),\n",
       " ('justiça', 70),\n",
       " ('ação', 69)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "philWordCount.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks slightly better. We still have words in Greek and Portuguese, but mostly we have names which we are interested in correcting. We can save the philosophy dictionary to file, add these word frequencies into our general dictionary and save the dictionary to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp/philosophyDict.json', 'w') as fp:\n",
    "    json.dump(philWordCount, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "customDictionary.word_frequency.load_dictionary('temp/philosophyDict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "customDictionary.export('../notebooks/wordlists/customDictionary.gz', gzipped = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
