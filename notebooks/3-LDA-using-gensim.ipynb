{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 3\n",
    "\n",
    "# Fitting an LDA to our corpus\n",
    "\n",
    "We plan to perform topic modeling using *Latent Dirichlet Allocation* (abbreviated as LDA). An LDA is a *generative model* that learns a group of categories (or *topics*) for words that occur together in a corpus of documents. For a technical presentation of LDAs, see [Appendix A](404).\n",
    "\n",
    "Let's start loading up our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.corpus import Corpus\n",
    "\n",
    "corpus = Corpus(registry_path = 'utils/article_registry.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate an initial `Model` object and give it access to our corpus. We must also give it the number of topics it should train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import Model\n",
    "\n",
    "n_topics = 10\n",
    "base_model = Model(corpus, n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the `Model`, we can use the `train()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this model to a file using the `save()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Model Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherence is an important statistic to compute in order to calibrate how many topics should we have on our final model. We can compute coherence by calling the `get_coherence()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coherence score allows us to do a search for the \"best\" `n_topics`. Notice that this coherence score is sensitive to the random number generation that is used when creating the `lda`. If we wanted to control this randomness, we can pass a `seed` parameter to the `train()` method. We will do this later when we implement our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a more complete grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last section shows we can compare different models and calibrate an optimal number of topics by training several models on a given number of topics. Now we will implement this experiment using a `gridsearch()` function. This function also makes use of the `get_stats()` method we included for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Run it again after addressing this comment: https://github.com/RaRe-Technologies/gensim/issues/2115#issuecomment-443113360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(min_topics, max_topics, step, iterations=3, verbose=True):\n",
    "    \"\"\"\n",
    "    Computes an array where we store statistics for each model. We run a search\n",
    "    n number of times per number of topics and record a set of statistics for each model.\n",
    "    At the end we will have n models per number of topics to compare.\n",
    "\n",
    "    Returns an array of the following form:\n",
    "\n",
    "    experiment = {\n",
    "        n_topics: {\n",
    "            0: [model(n_topics = 0).get_stats * iterations],\n",
    "            1: [model(n_topics = 1).get_stats * iterations],\n",
    "            ...\n",
    "            iterations - 1: [model(n_topics = iterations - 1).get_stats()]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    We expect all these inner model_stats() to be slightly different\n",
    "    due to stochasticity in the models.\n",
    "    \"\"\"\n",
    "    \n",
    "    experiment = {}\n",
    "    for n_topics in range(min_topics, max_topics, step):\n",
    "        experiment[n_topics] = {}\n",
    "        print(f\"\\nRunning experiment for {n_topics} topics.\")\n",
    "        print(\"----------\")\n",
    "        for i in range(iterations):\n",
    "            if verbose:\n",
    "                print(f\"Iteration: {i}\")\n",
    "\n",
    "            experiment[n_topics][i] = Model(corpus, n_topics).get_stats()\n",
    "\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Careful:** this gridsearch can take a whole evening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = gridsearch(80, 200, 10, iterations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save it for further analysis later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the gridsearch results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for n_topics in experiment:\n",
    "    for iteration, results in experiment[n_topics].items():\n",
    "        results['n_topics'] = n_topics\n",
    "        results['iteration'] = iteration\n",
    "        data.append(results)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['time_lda'] = df['time_lda'] / 60\n",
    "df['time_coherence'] = df['time_coherence'] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('../data/gridsearch.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('n_topics').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['coherence', 'log_perplexity', 'time_lda', 'time_coherence', 'avg_arts_per_topic', 'std_arts_per_topic']\n",
    "rows = 3\n",
    "cols = 2\n",
    "fig, axs = plt.subplots(rows, cols, sharex=True)\n",
    "\n",
    "for i, var in enumerate(vars):    \n",
    "    col = i % cols\n",
    "    row = i % rows\n",
    "    sns.lineplot(data=df, x='n_topics', y=var, ax=axs[row, col])\n",
    "    sns.scatterplot(data=df, x='n_topics', y=var, ax=axs[row, col])\n",
    "\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal topics seems to be 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reload the gridsearch and study what happens around 90 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/gridsearch.json\") as fp:\n",
    "    g = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g[\"90\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in g:\n",
    "    for v in g[k].values():\n",
    "        if \"-1\" in v[\"n_articles_per_topic\"]:\n",
    "            print(f\"Number of topics: {k}\")\n",
    "            print(\"Number of articles without 1st topic:\")\n",
    "            print(v[\"n_articles_per_topic\"][\"-1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 100 onwards, 254 articles get systematically thrown to 0 topics. Weird!\n",
    "\n",
    "90 doesn't have the no-topics-for-article problem, should we stick with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sticking with 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading online, people recommend that we save our dictionary in order to prevent randomness in it in the future. I will also set up the seed for the LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been 47,984 days since Wittgetstein was born (as of today, 09/09/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Model(corpus, 90)\n",
    "final_model.train(seed = 47984)\n",
    "final_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_model.get_stats())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d200da13d3d6a3f0988eb941b90abc0c57177e51b3c73553e7bc08a3e24cf245"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('journal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
