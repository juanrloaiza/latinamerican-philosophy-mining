{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 3\n",
    "\n",
    "# Fitting an LDA to our corpus\n",
    "\n",
    "We plan to perform topic modeling using *Latent Dirichlet Allocation* (abbreviated as LDA). An LDA is a *generative model* that learns a group of categories (or *topics*) for words that occur together in a corpus of documents. For a technical presentation of LDAs, see [Appendix A](404).\n",
    "\n",
    "Let's start loading up our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.corpus import Corpus\n",
    "\n",
    "corpus = Corpus(registry_path = 'utils/article_registry.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate an initial `Model` object and give it access to our corpus. We must also give it the number of topics it should train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus. Num. of articles: 906\n"
     ]
    }
   ],
   "source": [
    "from utils.model import Model\n",
    "\n",
    "n_topics = 10\n",
    "base_model = Model(corpus, n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the `Model`, we can use the `train()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bags of words collected. Starting training...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/3-LDA-using-gensim.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/3-LDA-using-gensim.ipynb#ch0000005?line=0'>1</a>\u001b[0m base_model\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py:37\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, seed, workers)\u001b[0m\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBags of words collected. Starting training...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=27'>28</a>\u001b[0m \u001b[39m\"\"\" self.lda = LdaMulticore(\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=28'>29</a>\u001b[0m \u001b[39m    corpus_bows,\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=29'>30</a>\u001b[0m \u001b[39m    num_topics=self.num_topics,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=33'>34</a>\u001b[0m \u001b[39m    workers=workers,\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=34'>35</a>\u001b[0m \u001b[39m) \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=36'>37</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlda \u001b[39m=\u001b[39m LdaSeqModel(\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=37'>38</a>\u001b[0m     corpus\u001b[39m=\u001b[39;49mcorpus_bows,\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=38'>39</a>\u001b[0m     time_slice\u001b[39m=\u001b[39;49m[\u001b[39m300\u001b[39;49m, \u001b[39m300\u001b[39;49m, \u001b[39m300\u001b[39;49m],\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=39'>40</a>\u001b[0m     num_topics\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_topics,\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=40'>41</a>\u001b[0m     id2word\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid2word,\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=41'>42</a>\u001b[0m     passes\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=42'>43</a>\u001b[0m     random_state\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=43'>44</a>\u001b[0m )\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel trained! Creating Topic objects...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///home/juanrloaiza/JournalMiningProject/latinamerican-philosophy-mining/notebooks/utils/model.py?line=46'>47</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_topics()\n",
      "File \u001b[0;32m~/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py:192\u001b[0m, in \u001b[0;36mLdaSeqModel.__init__\u001b[0;34m(self, corpus, time_slice, id2word, alphas, num_topics, initialize, sstats, lda_model, obs_variance, chain_variance, passes, random_state, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)\u001b[0m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=188'>189</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_ldaseq_ss(chain_variance, obs_variance, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphas, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msstats)\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=190'>191</a>\u001b[0m \u001b[39m# fit DTM\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=191'>192</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)\n",
      "File \u001b[0;32m~/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py:281\u001b[0m, in \u001b[0;36mLdaSeqModel.fit_lda_seq\u001b[0;34m(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)\u001b[0m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=276'>277</a>\u001b[0m lhoods \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((corpus_len, num_topics \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=277'>278</a>\u001b[0m \u001b[39m# compute the likelihood of a sequential corpus under an LDA\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=278'>279</a>\u001b[0m \u001b[39m# seq model and find the evidence lower bound. This is the E - Step\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=279'>280</a>\u001b[0m bound, gammas \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=280'>281</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=281'>282</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgammas \u001b[39m=\u001b[39m gammas\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=283'>284</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mM Step\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py:355\u001b[0m, in \u001b[0;36mLdaSeqModel.lda_seq_infer\u001b[0;34m(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\u001b[0m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=352'>353</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDTM\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDTM\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=354'>355</a>\u001b[0m     bound, gammas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minferDTMseq(\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=355'>356</a>\u001b[0m         corpus, topic_suffstats, gammas, lhoods, lda,\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=356'>357</a>\u001b[0m         ldapost, iter_, bound, lda_inference_max_iter, chunksize\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=357'>358</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=358'>359</a>\u001b[0m \u001b[39melif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDIM\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=359'>360</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mInfluenceTotalFixed(corpus)\n",
      "File \u001b[0;32m~/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py:417\u001b[0m, in \u001b[0;36mLdaSeqModel.inferDTMseq\u001b[0;34m(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\u001b[0m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=414'>415</a>\u001b[0m \u001b[39mif\u001b[39;00m doc_index \u001b[39m>\u001b[39m time_slice[time]:\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=415'>416</a>\u001b[0m     time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=416'>417</a>\u001b[0m     lda \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_lda_seq_slice(lda, time)  \u001b[39m# create lda_seq slice\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=417'>418</a>\u001b[0m     doc_num \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=419'>420</a>\u001b[0m gam \u001b[39m=\u001b[39m gammas[doc_index]\n",
      "File \u001b[0;32m~/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py:465\u001b[0m, in \u001b[0;36mLdaSeqModel.make_lda_seq_slice\u001b[0;34m(self, lda, time)\u001b[0m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=447'>448</a>\u001b[0m \u001b[39m\"\"\"Update the LDA model topic-word values using time slices.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=448'>449</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=449'>450</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=461'>462</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=462'>463</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=463'>464</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_topics):\n\u001b[0;32m--> <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=464'>465</a>\u001b[0m     lda\u001b[39m.\u001b[39mtopics[:, k] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtopic_chains[k]\u001b[39m.\u001b[39;49me_log_prob[:, time]\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=466'>467</a>\u001b[0m lda\u001b[39m.\u001b[39malpha \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphas)\n\u001b[1;32m    <a href='file:///home/juanrloaiza/.miniconda3/envs/journal/lib/python3.9/site-packages/gensim/models/ldaseqmodel.py?line=467'>468</a>\u001b[0m \u001b[39mreturn\u001b[39;00m lda\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "base_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this model to a file using the `save()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Model Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherence is an important statistic to compute in order to calibrate how many topics should we have on our final model. We can compute coherence by calling the `get_coherence()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coherence score allows us to do a search for the \"best\" `n_topics`. Notice that this coherence score is sensitive to the random number generation that is used when creating the `lda`. If we wanted to control this randomness, we can pass a `seed` parameter to the `train()` method. We will do this later when we implement our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a more complete grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last section shows we can compare different models and calibrate an optimal number of topics by training several models on a given number of topics. Now we will implement this experiment using a `gridsearch()` function. This function also makes use of the `get_stats()` method we included for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Run it again after addressing this comment: https://github.com/RaRe-Technologies/gensim/issues/2115#issuecomment-443113360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(min_topics, max_topics, step, iterations=3, verbose=True):\n",
    "    \"\"\"\n",
    "    Computes an array where we store statistics for each model. We run a search\n",
    "    n number of times per number of topics and record a set of statistics for each model.\n",
    "    At the end we will have n models per number of topics to compare.\n",
    "\n",
    "    Returns an array of the following form:\n",
    "\n",
    "    experiment = {\n",
    "        n_topics: {\n",
    "            0: [model(n_topics = 0).get_stats * iterations],\n",
    "            1: [model(n_topics = 1).get_stats * iterations],\n",
    "            ...\n",
    "            iterations - 1: [model(n_topics = iterations - 1).get_stats()]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    We expect all these inner model_stats() to be slightly different\n",
    "    due to stochasticity in the models.\n",
    "    \"\"\"\n",
    "    \n",
    "    experiment = {}\n",
    "    for n_topics in range(min_topics, max_topics, step):\n",
    "        experiment[n_topics] = {}\n",
    "        print(f\"\\nRunning experiment for {n_topics} topics.\")\n",
    "        print(\"----------\")\n",
    "        for i in range(iterations):\n",
    "            if verbose:\n",
    "                print(f\"Iteration: {i}\")\n",
    "\n",
    "            experiment[n_topics][i] = Model(corpus, n_topics).get_stats()\n",
    "\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Careful:** this gridsearch can take a whole evening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = gridsearch(80, 200, 10, iterations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save it for further analysis later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the gridsearch results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for n_topics in experiment:\n",
    "    for iteration, results in experiment[n_topics].items():\n",
    "        results['n_topics'] = n_topics\n",
    "        results['iteration'] = iteration\n",
    "        data.append(results)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['time_lda'] = df['time_lda'] / 60\n",
    "df['time_coherence'] = df['time_coherence'] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('../data/gridsearch.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('n_topics').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['coherence', 'log_perplexity', 'time_lda', 'time_coherence', 'avg_arts_per_topic', 'std_arts_per_topic']\n",
    "rows = 3\n",
    "cols = 2\n",
    "fig, axs = plt.subplots(rows, cols, sharex=True)\n",
    "\n",
    "for i, var in enumerate(vars):    \n",
    "    col = i % cols\n",
    "    row = i % rows\n",
    "    sns.lineplot(data=df, x='n_topics', y=var, ax=axs[row, col])\n",
    "    sns.scatterplot(data=df, x='n_topics', y=var, ax=axs[row, col])\n",
    "\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal topics seems to be 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reload the gridsearch and study what happens around 90 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/gridsearch.json\") as fp:\n",
    "    g = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g[\"90\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in g:\n",
    "    for v in g[k].values():\n",
    "        if \"-1\" in v[\"n_articles_per_topic\"]:\n",
    "            print(f\"Number of topics: {k}\")\n",
    "            print(\"Number of articles without 1st topic:\")\n",
    "            print(v[\"n_articles_per_topic\"][\"-1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 100 onwards, 254 articles get systematically thrown to 0 topics. Weird!\n",
    "\n",
    "90 doesn't have the no-topics-for-article problem, should we stick with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sticking with 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading online, people recommend that we save our dictionary in order to prevent randomness in it in the future. I will also set up the seed for the LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been 47,984 days since Wittgetstein was born (as of today, 09/09/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Model(corpus, 90)\n",
    "final_model.train(seed = 47984)\n",
    "final_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_model.get_stats())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d200da13d3d6a3f0988eb941b90abc0c57177e51b3c73553e7bc08a3e24cf245"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('journal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
