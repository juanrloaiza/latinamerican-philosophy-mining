{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Preprocessing 1\n",
    "# Artifact Removal and Stopword Selection\n",
    "This notebook takes the clean JSON files for each article and does some preprocessing to obtain a text that we can analyze using LDA.\n",
    "\n",
    "Specifically, we do:\n",
    "* Artifact removal\n",
    "* Stopword selection\n",
    "\n",
    "In the next notebook we will do:\n",
    "* Punctuation removal\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use a utility `Corpus` class defined in the `utils/` folder.\n",
    "\n",
    "The `Corpus` class includes methods to get the documents, which we get as `Article` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.corpus import Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(registry_path='utils/article_registry.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only work with documents in Spanish, with text, and that are indeed articles (and not reviews, for example). The `Corpus` class included a method with these criteria by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus. Num. of articles: 771\n"
     ]
    }
   ],
   "source": [
    "corpusList = corpus.get_documents_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves is with approximately 700 articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for duplicates in case we have some. This is important in case we downloaded an article twice, once in PDF and once in HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([doc.id for doc in corpusList]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifact removal\n",
    "\n",
    "There are some artifacts included in the text that are produced by HTML and PDF processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by removing numbers and some special characters such as newline characters (`\\n`). We will keep normal punctuation for now as that might help SpaCy when we do lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpusList:\n",
    "    doc.clean_text = re.sub('\\d|\\n',' ', doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can detect some of these artifacts by looking for non-alphanumeric characters between alphanumeric characters (e.g. `\"ar-gument\"`, `\"ar\\xadgument\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = re.compile('\\w+[^a-zA-ZáéíóúÁÉÍÓÚñÑüÜ\\d\\s:]\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first-order', \"Taylor's\", \"Taylor's\", '29-40', '18-19']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.findall(artifacts, doc.text) for doc in corpusList][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common artifact is the hex `\\xad` for the soft hyphen which is used to break lines. We can remove it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpusList:\n",
    "    doc.clean_text = re.sub('\\\\xad','', doc.clean_text)\n",
    "    doc.clean_text = doc.clean_text.replace(u\"\\xa0\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the corpus for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.save_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Selection\n",
    "Stopword removal is perhaps the most difficult part of preprocessing. There are two challenges to meet:\n",
    "* Some stopword lists such as the one included in NLTK for Spanish are too weak and do not filter many stopwords.\n",
    "* Other stopword lists are too inclusive and can eliminate words that are meaningful in philosophy (e.g. 'verdadero', true). \n",
    "It is important to note that stopwords are very context-sensitive. A word in one context may provide little meaning (hence counting as a stopword) while in other contexts it may provide lots of information.\n",
    "\n",
    "To tackle these challenges, we will first to an initial filtering with NTLK's list. This will leave many stopwords in the text, but will reduce the size of each text considerably. Then we will compare the text with a stronger list of stopwords (source). We will see which words are both in the text and the stronger stopwords list. We will inspect these lists manually and extract a list of protected words. We will iterate over this process a number of times. Once we have a robust list of protected words, we will concatenate NLTK's stopwords list with the stronger one and eliminate the protected words from it. This will provide a final (hopefully middle ground) stopword list with which to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "stopwords_weak = nltk_stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://raw.githubusercontent.com/stopwords-iso/stopwords-es/master/stopwords-es.txt')\n",
    "stopwords_strong = r.text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 3489552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xml',\n",
       " 'recibido',\n",
       " 'febrero',\n",
       " 'aceptado',\n",
       " 'abril',\n",
       " 'resumen',\n",
       " 'artículo',\n",
       " 'sostiene',\n",
       " 'consenso',\n",
       " 'tema']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_words = []\n",
    "for doc in corpusList:\n",
    "    doc_words = [word.lower() for word in re.findall('\\w+', doc.clean_text) if len(word) > 1]\n",
    "    corpus_words += [word for word in doc_words if word not in stopwords_weak]\n",
    "\n",
    "print(f\"Total words: {len(corpus_words):d}\")\n",
    "corpus_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "realwords_and_stopwords = Counter([word for word in corpus_words if word in stopwords_strong])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ser', 22744),\n",
       " ('si', 17875),\n",
       " ('puede', 15515),\n",
       " ('sino', 12538),\n",
       " ('mismo', 11815),\n",
       " ('bien', 9461),\n",
       " ('decir', 8667),\n",
       " ('así', 8428),\n",
       " ('tal', 7662),\n",
       " ('modo', 7587)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realwords_and_stopwords.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already in the first 10 most common words in both the documents and the strong list of stopwords we find words that in philosophy are quite meaningful:\n",
    "* 'ser': being\n",
    "* 'bien': good\n",
    "* 'modo': mode\n",
    "\n",
    "We will start saving those words and eliminating them from the stronger list of stopwords. Then we will repeat the process of selecting the words that are in both lists and see which words are common. By iterating over this process a couple of times, we will get a list of protected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "protected_words = ['bien']\n",
    "new_protected_words = ['ser']\n",
    "checked_words = []\n",
    "\n",
    "empty_iterations = 0\n",
    "max_iterations = 3\n",
    "while empty_iterations < max_iterations:\n",
    "    candidates = Counter({word: count for word, count in realwords_and_stopwords.items() if word not in checked_words})\n",
    "    candidates = candidates.most_common(20)\n",
    "\n",
    "    checked_words += [word for word, count in candidates]\n",
    "\n",
    "    print(candidates)\n",
    "\n",
    "    new_protected_words = input(\"New protected words (comma separated) [end with 'None']: \")\n",
    "    protected_words += new_protected_words.split(', ')\n",
    "    if new_protected_words == 'None':\n",
    "        empty_iterations += 1\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we have added words that we observed were incorrectly lemmatized. We will pass the list of protected words to the lemmatizer later on and we will skip these protected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_words = [word for word in protected_words if word and word != 'None']\n",
    "protected_words = list(set(protected_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords in other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that most of the articles have abstracts in English, some of the usual stopwords in English are appearing frequently in our documents. Thus, we will append the NLTK-generated list of English stopwords. We will also use one for Portuguese, which we sometimes get as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = nltk_stopwords.words(\"english\")\n",
    "portuguese_stopwords = nltk_stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: should we be filtering these just like we filter the Spanish ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other stopwords custom to our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other stopwords that we would like to include, but that have not been taken into account in the previous processes. These are found in `wordlists/custom_stopwords.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a robust set of words we can save both the final stopword list and the protected words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordlists/custom_stopwords.txt') as file:\n",
    "    custom_stopwords = file.read().split()\n",
    "\n",
    "stopwords_final = list(set(\n",
    "    stopwords_weak + stopwords_strong + english_stopwords + portuguese_stopwords +  custom_stopwords\n",
    "))\n",
    "with open('wordlists/stopwords.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join(stopwords_final))\n",
    "\n",
    "with open('wordlists/protectedWords.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join(protected_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.save_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final replacements and edits\n",
    "PDF correction is not perfect and we observe some artifacts left in the LDA. A hotfix is to do those replacements manually for now and check whether we can improve on this process in the future.\n",
    "\n",
    "Note: I saved these and removed the cells which contained this dictionary. We can find it in `wordlists/old_manual_replacements.json`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
