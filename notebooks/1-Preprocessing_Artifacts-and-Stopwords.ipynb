{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Preprocessing 1\n",
    "# Artifact Removal and Stopword Selection\n",
    "This notebook takes the clean JSON files for each article and does some preprocessing to obtain a text that we can analyze using LDA.\n",
    "\n",
    "Specifically, we do:\n",
    "* Artifact removal\n",
    "* Stopword selection\n",
    "\n",
    "In the next notebook we will do:\n",
    "* Punctuation removal\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use some utility functions we defined in the `utils/` folder:\n",
    "* `loadCorpusList(path)`: Loads the corpus as a list of `Article` objects (see `utils/Article.py`). This will allow us to save the clean text per document into the same JSON file with the metadata included.\n",
    "* `saveCorpus(path)`: Saves the articles in JSON format in their current state. Useful when we want to append information to our clean JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "# Jupyter Notebooks are not good at handling relative imports.\n",
    "# Best solution (not great practice) is to add the project's path\n",
    "# to the module loading paths of sys.\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.loaders import loadCorpusList, saveCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusPath = '../data/parsedHTML'\n",
    "\n",
    "corpusList = loadCorpusList(corpusPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"La diferencia 'es/debe' y el argumento de la pregunta cerrada\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusList[0].title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only work with documents in Spanish. Hence, let's replace `corpusList` with only the articles that are in Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusList = [doc for doc in corpusList if doc.lang == 'es']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves is with 554 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpusList) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifact removal\n",
    "\n",
    "There are some artifacts included in the text that are produced by HTML processing (or in the future because of how PDF files store text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by removing numbers and some special characters such as newline characters (`\\n`). We will keep normal punctuation for now as that might help SpaCy when we do lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpusList:\n",
    "    doc.cleanText = re.sub('\\d|\\n',' ', doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can detect some of these artifacts by looking for non-alphanumeric characters between alphanumeric characters (e.g. `\"ar-gument\"`, `\"ar\\xadgument\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = re.compile('\\w+[^a-zA-ZáéíóúÁÉÍÓÚñÑüÜ\\d\\s:]\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ES/DEBE',\n",
       " 'G.E',\n",
       " 'auto-responde',\n",
       " 'auto-respond',\n",
       " 'no-natura',\n",
       " 'no-naturale',\n",
       " 'ES/DEBE',\n",
       " 'es/debe',\n",
       " 'es/deb',\n",
       " 'no-valorativo',\n",
       " 'afirmacic»ies',\n",
       " 'ES/DEBE',\n",
       " 'no-concluyente',\n",
       " 'ai;giiment',\n",
       " 'auto-respond',\n",
       " 'no-concluyent',\n",
       " 'auto-responde',\n",
       " 'no-concluyentes',\n",
       " 'hecho/valo',\n",
       " 'ES/DEBE',\n",
       " 'no-concluyente',\n",
       " 'auto-respond',\n",
       " 'no-concluyente',\n",
       " 'es/deb',\n",
       " 'no-concluyente',\n",
       " 'G.E',\n",
       " 'R.M']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.findall(artifacts, doc.text) for doc in corpusList][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common artifact is the hex `\\xad` for the soft hyphen which is used to break lines. We can remove it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpusList:\n",
    "    doc.cleanText = re.sub('\\\\xad','', doc.cleanText)\n",
    "    doc.cleanText = doc.cleanText.replace(u\"\\xa0\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the corpus for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCorpus('../data/corpus', corpusList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "Stopword removal is perhaps the most difficult part of preprocessing. There are two challenges to meet:\n",
    "* Some stopword lists such as the one included in NLTK for Spanish are too weak and do not filter many stopwords.\n",
    "* Other stopword lists are too inclusive and can eliminate words that are meaningful in philosophy (e.g. 'verdadero', true). \n",
    "It is important to note that stopwords are very context-sensitive. A word in one context may provide little meaning (hence counting as a stopword) while in other contexts it may provide lots of information.\n",
    "\n",
    "To tackle these challenges, we will first to an initial filtering with NTLK's list. This will leave many stopwords in the text, but will reduce the size of each text considerably. Then we will compare the text with a stronger list of stopwords (source). We will see which words are both the text and the stronger stopwords list. We will inspect these lists manually and extract a list of protected words. We will iterate over this process a number of times. Once we have a robust list of protected words, we will concatenate NLTK's stopwords list with the stronger one and eliminate the protected words from it. This will provide a final (hopefully middle ground) stopword list with which to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "stopwords_weak = nltk_stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://raw.githubusercontent.com/stopwords-iso/stopwords-es/master/stopwords-es.txt')\n",
    "stopwords_strong = r.text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docWords = []\n",
    "for doc in corpusList:\n",
    "    docWords += [word for word in re.findall('\\w+', doc.cleanText) if word not in stopwords_weak]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L', 'A', 'DIFERENCI', 'A', 'ES', 'DEBE', 'Y', 'E', 'L', 'ARGUMENT']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "docs_and_stopwords = Counter([word for word in docWords if word in stopwords_strong])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 169541),\n",
       " ('l', 132270),\n",
       " ('n', 130995),\n",
       " ('d', 95875),\n",
       " ('r', 54215),\n",
       " ('u', 21476),\n",
       " ('ser', 17683),\n",
       " ('si', 12842),\n",
       " ('puede', 12369),\n",
       " ('i', 9685)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_stopwords.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already in the first 10 most common words in both the documents and the strong list of stopwords we find words that in philosophy are quite meaningful:\n",
    "* 'ser': being\n",
    "* 'bien': good\n",
    "* 'modo': mode\n",
    "\n",
    "We will start saving those words and eliminating them from the stronger list of stopwords. Then we will repeat the process of selecting the words that are in both lists and see which words are common. By iterating over this process a couple of times, we will get a list of protected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "protectedWords = [\n",
    "    'ser',\n",
    "    'bien',\n",
    "    'modo'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_strong = [word for word in stopwords_strong if word not in protectedWords]\n",
    "docs_and_stopwords = Counter([word for word in docWords if word in stopwords_strong])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 169541),\n",
       " ('l', 132270),\n",
       " ('n', 130995),\n",
       " ('d', 95875),\n",
       " ('r', 54215),\n",
       " ('u', 21476),\n",
       " ('si', 12842),\n",
       " ('puede', 12369),\n",
       " ('i', 9685),\n",
       " ('mismo', 9186),\n",
       " ('sino', 9169),\n",
       " ('t', 7809),\n",
       " ('decir', 7218),\n",
       " ('parte', 6280),\n",
       " ('manera', 6272),\n",
       " ('dos', 5629),\n",
       " ('tal', 5395),\n",
       " ('hace', 5326),\n",
       " ('pues', 5056),\n",
       " ('así', 5047),\n",
       " ('misma', 4764),\n",
       " ('lugar', 4676),\n",
       " ('debe', 4663),\n",
       " ('embargo', 4656),\n",
       " ('posible', 4616),\n",
       " ('sólo', 4584),\n",
       " ('hecho', 4507),\n",
       " ('ejemplo', 4278),\n",
       " ('vez', 4246),\n",
       " ('p', 4190),\n",
       " ('entonces', 4164),\n",
       " ('verdad', 4115),\n",
       " ('tiempo', 4013),\n",
       " ('solo', 3889),\n",
       " ('ello', 3826),\n",
       " ('según', 3703),\n",
       " ('toda', 3665),\n",
       " ('m', 3497),\n",
       " ('h', 3475),\n",
       " ('cosas', 3458),\n",
       " ('poder', 3453),\n",
       " ('parece', 3306),\n",
       " ('b', 3304),\n",
       " ('cada', 3254),\n",
       " ('respecto', 3182),\n",
       " ('pueden', 3180),\n",
       " ('saber', 3165),\n",
       " ('aquí', 3152),\n",
       " ('da', 3120),\n",
       " ('f', 3110),\n",
       " ('hacer', 3096),\n",
       " ('primer', 3010),\n",
       " ('general', 3000),\n",
       " ('primera', 2963),\n",
       " ('partir', 2963),\n",
       " ('menos', 2942),\n",
       " ('cómo', 2906),\n",
       " ('trata', 2869),\n",
       " ('siempre', 2816),\n",
       " ('podría', 2794),\n",
       " ('z', 2672),\n",
       " ('cuanto', 2655),\n",
       " ('propia', 2639),\n",
       " ('propio', 2489),\n",
       " ('fin', 2461),\n",
       " ('tener', 2351),\n",
       " ('dice', 2321),\n",
       " ('ver', 2271),\n",
       " ('podemos', 2232),\n",
       " ('todas', 2227),\n",
       " ('sido', 2225),\n",
       " ('momento', 2218),\n",
       " ('cierto', 2203),\n",
       " ('medio', 2166),\n",
       " ('tan', 2149),\n",
       " ('cuenta', 2139),\n",
       " ('aunque', 2107),\n",
       " ('lado', 2097),\n",
       " ('hacia', 2068),\n",
       " ('segundo', 2067),\n",
       " ('cualquier', 2057),\n",
       " ('acuerdo', 2030),\n",
       " ('trabajo', 1980),\n",
       " ('través', 1948),\n",
       " ('alguna', 1893),\n",
       " ('aquello', 1857),\n",
       " ('bajo', 1844),\n",
       " ('mejor', 1824),\n",
       " ('incluso', 1803),\n",
       " ('diferentes', 1798),\n",
       " ('dicho', 1793),\n",
       " ('dado', 1788),\n",
       " ('dentro', 1775),\n",
       " ('último', 1762),\n",
       " ('uso', 1754),\n",
       " ('ahora', 1718),\n",
       " ('pueda', 1662),\n",
       " ('encuentra', 1656),\n",
       " ('cuales', 1626),\n",
       " ('c', 1615)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_stopwords.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "protectedWords += [\n",
    "    'parte',\n",
    "    'posible',\n",
    "    'lugar',\n",
    "    'hecho',\n",
    "    'poder',\n",
    "    'verdad',\n",
    "    'cosas',\n",
    "    'general',\n",
    "    'fin',\n",
    "    'trabajo',\n",
    "    'cierto',\n",
    "    'uso',\n",
    "    'dado',\n",
    "    'diferentes',\n",
    "    'verdadero',\n",
    "    'verdadera',\n",
    "    'existe',\n",
    "    'valor',\n",
    "    'realizar',\n",
    "    'existen',\n",
    "    'conocer',\n",
    "    'diferente'\n",
    "]\n",
    "\n",
    "protectedWords = list(set(protectedWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a couple of times to make the process less complex, once we are sure of a set of words, we can eliminate those from the list of document words and go back and repeat the process a couple more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsToRemove = [word[0] for word in docs_and_stopwords.most_common(100)]\n",
    "docWords = [word for word in docWords if word not in stopwordsToRemove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords in English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that most of the articles have abstracts in English, some of the usual stopwords in English are appearing frequently in our documents. Thus, we will append the NLTK-generated list of English stopwords. We will also use one for Portuguese, which we sometimes get as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishStopwords = nltk_stopwords.words(\"english\")\n",
    "portugueseStopwords = nltk_stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: should we be filtering these just like we filter the Spanish ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other stopwords custom to our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other stopwords that we would like to include, but that have not been taken into account in the previous processes. These are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = [\n",
    "    \"cf\",\n",
    "    \"no obstante\",\n",
    "    \"sin embargo\",\n",
    "    \"por ejemplo\",\n",
    "    \"es decir\",\n",
    "    \"ak\",\n",
    "    \"krv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a robust set of words we can save both the final stopword list and the protected words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_final = list(set(\n",
    "    stopwords_weak + stopwords_strong + englishStopwords + portugueseStopwords +  custom_stopwords\n",
    "))\n",
    "with open('wordlists/stopwords.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join(stopwords_final))\n",
    "\n",
    "with open('wordlists/protectedWords.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join(protectedWords))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
