{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Preprocessing 2\n",
    "# Stopword and punctuation removal, lemmatization\n",
    "\n",
    "This notebook uses the lists of stopwords and protected words discussed in the previous notebook to clean the documents and arrive at a bag-of-words presentation of all documents. In this presentation, we stick with just the lemmata of all words that aren't stopwords.\n",
    "\n",
    "After running this notebook, all json documents will contain a new entry `bagOfWords` containing the clean representation, ready for `scikit-learn`s LDA. We will save these entries using our utility function defined in `util.loaders`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start this notebook by loading the corpus just as in the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys \n",
    "\n",
    "# Jupyter Notebooks are not good at handling relative imports.\n",
    "# Best solution (not great practice) is to add the project's path\n",
    "# to the module loading paths of sys.\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.loaders import loadCorpusList, saveCorpus\n",
    "\n",
    "corpusPath = '../data/corpus'\n",
    "\n",
    "corpusList = loadCorpusList(corpusPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusList = [doc for doc in corpusList if doc.lang == \"es\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1330"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpusList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the wordlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the stopwords we defined, the protected words, and the dictionary of our manual lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wordlists/stopwords.txt\") as fp:\n",
    "    stopwords = fp.read()\n",
    "\n",
    "with open(\"wordlists/protectedWords.txt\") as fp:\n",
    "    protectedWords = fp.read()\n",
    "    \n",
    "with open(\"wordlists/manualLemmas.txt\") as fp:\n",
    "    manualLemmas = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.split(\"\\n\"))\n",
    "protectedWords = set(protectedWords.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Spacy and its spanish model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using SpaCy, we will need to install their `es_core_news_md` Natural Language Processing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/models:\n",
    "# Run the next line to install the NLP model from SpaCy.\n",
    "# !python -m spacy download es_core_news_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import es_core_news_md\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a generic function that cleans the documents in the `corpusList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article(article):\n",
    "    \"\"\"\n",
    "    TODO: write this.\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanText = article.cleanText\n",
    "    table = str.maketrans('', '', string.punctuation + \"¡¿\")\n",
    "    cleanText = cleanText.translate(table).lower()\n",
    "    \n",
    "    # Cleaning compound stopwords\n",
    "    for stopword in stopwords:\n",
    "        if len(stopword.split(\" \")) > 1:\n",
    "            cleanText = cleanText.replace(stopword, \"\")\n",
    "    \n",
    "    # Getting the bag of words representation\n",
    "    bagOfWords = []\n",
    "    for token in nlp(cleanText):\n",
    "        # Ignore stopwords\n",
    "        if token.text in stopwords:\n",
    "            continue\n",
    "\n",
    "        # Protect some words\n",
    "        if token.text in protectedWords:\n",
    "            # If the word is in the manual lemmas, we replace.\n",
    "            # Otherwise, we just add the word.\n",
    "            if token.text in manualLemmas.keys():\n",
    "                bagOfWords.append(manualLemmas[token.text])\n",
    "            else:\n",
    "                bagOfWords.append(token.text)\n",
    "\n",
    "        # For the rest, store lemmatas\n",
    "        else:\n",
    "            bagOfWords.append(token.lemma_)\n",
    "    \n",
    "    # Add the atribute to articles.\n",
    "    # TODO: some of the w here are spaces, I should\n",
    "    # remove them.\n",
    "    bagOfWords = [w for w in bagOfWords if w != \"\"]\n",
    "    bagOfWords = \" \".join(bagOfWords)\n",
    "    article.bagOfWords = bagOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in corpusList:\n",
    "    clean_article(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I could run this in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "with Pool(5) as pool:\n",
    "    pool.map(clean_article, corpusList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCorpus('../data/corpus', corpusList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
