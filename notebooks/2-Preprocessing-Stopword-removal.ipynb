{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Preprocessing 2\n",
    "# Stopword and punctuation removal, lemmatization\n",
    "\n",
    "This notebook uses the lists of stopwords and protected words discussed in the previous notebook to clean the documents and arrive at a bag-of-words presentation of all documents. In this presentation, we stick with just the lemmata of all words that aren't stopwords.\n",
    "\n",
    "After running this notebook, all json documents will contain a new entry `bagOfWords` containing the clean representation, ready for `scikit-learn`s LDA. We will save these entries using our utility function defined in `util.loaders`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start this notebook by loading the corpus just as in the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus. Num. of articles: 877\n"
     ]
    }
   ],
   "source": [
    "from utils.corpus import Corpus\n",
    "\n",
    "corpus = Corpus(registry_path='utils/article_registry.json')\n",
    "corpus_list = corpus.get_documents_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the wordlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the stopwords we defined, the protected words, and the dictionary of our manual lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"wordlists/stopwords.txt\") as fp:\n",
    "    stopwords = fp.read()\n",
    "\n",
    "with open(\"wordlists/protectedWords.txt\") as fp:\n",
    "    protected_words = fp.read()\n",
    "    \n",
    "with open(\"wordlists/manualLemmas.json\") as fp:\n",
    "    manual_lemmas = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.split(\"\\n\"))\n",
    "protected_words = set(protected_words.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Spacy and its spanish model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using SpaCy, we will need to install their `es_core_news_md` Natural Language Processing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/models:\n",
    "# Run the next line to install the NLP model from SpaCy.\n",
    "# !python -m spacy download es_core_news_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a generic function that cleans the documents in the `corpus_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_article(article):\n",
    "    \"\"\"\n",
    "    This function takes in an Article object (as constructed with the Article class (see utils)).\n",
    "    It processes the clean_text attribute of this object, which holds the text processed so far,\n",
    "    and implements the following steps:\n",
    "    \n",
    "        1. Short word removal: words that are less than 2 letters long are ignored.\n",
    "        2. Stopword removal: stopwords are also ignored.\n",
    "        3. Lemmatization: transforms each word into its corresponding lemma.\n",
    "        \n",
    "    It then saves the resulting bag of words into a bag_of_words attribute inside the Article object.\n",
    "    \n",
    "    Input: Article (object)\n",
    "    Return: None (function modifies object directly)\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_text = ' '.join(re.findall(\"\\w+\", article.clean_text)).lower()\n",
    "    \n",
    "    # Cleaning compound stopwords\n",
    "    for stopword in stopwords:\n",
    "        if len(stopword.split(\" \")) > 1:\n",
    "            clean_text = clean_text.replace(stopword, \"\")\n",
    "    \n",
    "    clean_text = ' '.join(re.findall(\"\\w+\", clean_text))\n",
    "\n",
    "    # Getting the bag of words representation\n",
    "    bag_of_words = []\n",
    "    for token in nlp(clean_text):        \n",
    "        # Ignore short words and stopwords\n",
    "        if (len(token.text) <= 2) or (token.text in stopwords):\n",
    "            continue\n",
    "            \n",
    "            # NOTE: \"Yo\" might be an imporant word. Which other 2-letter words are important?\n",
    "            # NOTE 2: Eliminating 2-letter words also helps distinguish \"es\" from \"ser\".\n",
    "\n",
    "        # Protect some words\n",
    "        if token.text in protected_words:            \n",
    "            bag_of_words.append(token.text)\n",
    "\n",
    "        # If the word is in the manual lemmas, we replace.\n",
    "        # Otherwise, we just add the word.\n",
    "        elif token.text in manual_lemmas:\n",
    "            bag_of_words.append(manual_lemmas[token.text])\n",
    "\n",
    "        # For the rest, store lemmatas\n",
    "        else:\n",
    "            bag_of_words.append(token.lemma_)\n",
    "    \n",
    "    # Add the atribute to articles.\n",
    "    bag_of_words = [w for w in bag_of_words if w != \"\"]\n",
    "    bag_of_words = \" \".join(bag_of_words)\n",
    "    \n",
    "    \n",
    "    # For some strange reason, there are weird blank characters in the bag of words,\n",
    "    # some of which are not regular spaces nor tabs nor line breaks. We implement an extra\n",
    "    # step that only includes word characters using regex before a final join.\n",
    "    bag_of_words = re.findall('\\w+', bag_of_words)\n",
    "    bag_of_words = [w for w in bag_of_words if len(w) > 2]\n",
    "    bag_of_words = \" \".join(bag_of_words)\n",
    "\n",
    "    article.bag_of_words = bag_of_words\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the cleaning process in parallel using Python's `multiprocessing` library. By default we use 5 threads, but this can be changed according to the available number of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "with Pool(5) as pool:\n",
    "    processed_articles = pool.map(clean_article, corpus_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the new corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the processed articles using the `save_documents()` method in the `Corpus` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.documents = processed_articles\n",
    "corpus.save_documents()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d200da13d3d6a3f0988eb941b90abc0c57177e51b3c73553e7bc08a3e24cf245"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('journal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
