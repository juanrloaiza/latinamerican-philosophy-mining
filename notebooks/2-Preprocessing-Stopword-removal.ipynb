{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Preprocessing 2\n",
    "# Stopword and punctuation removal, lemmatization\n",
    "\n",
    "This notebook uses the lists of stopwords and protected words discussed in the previous notebook to clean the documents and arrive at a bag-of-words presentation of all documents. In this presentations, we stick with just the lemmata of all words that aren't stopwords.\n",
    "\n",
    "After running this notebook, all json documents will contain a new entry `bagOfWords` containing the clean representation, ready for `scikit-learn`s LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start this notebook by loading the corpus just as in the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys \n",
    "\n",
    "# Jupyter Notebooks are not good at handling relative imports.\n",
    "# Best solution (not great practice) is to add the project's path\n",
    "# to the module loading paths of sys.\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.loaders import loadCorpusList, saveCorpus\n",
    "\n",
    "corpusPath = '../data/clean_json'\n",
    "\n",
    "corpusList = loadCorpusList(corpusPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusList = [doc for doc in corpusList if doc.lang == \"es\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the wordlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the stopwords we defined, the protected words, and the dictionary of our manual lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wordlists/stopwords.txt\") as fp:\n",
    "    stopwords = fp.read()\n",
    "\n",
    "with open(\"wordlists/protectedWords.txt\") as fp:\n",
    "    protectedWords = fp.read()\n",
    "    \n",
    "with open(\"wordlists/manualLemmas.txt\") as fp:\n",
    "    manualLemmas = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.split(\"\\n\"))\n",
    "protectedWords = set(protectedWords.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Spacy and its spanish model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using SpaCy, we will need to install their `es_core_news_md` Natural Language Processing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: es_core_news_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.2.5/es_core_news_md-2.2.5.tar.gz#egg=es_core_news_md==2.2.5 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from es_core_news_md==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (4.46.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (46.1.1.post20200323)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/juanrloaiza/miniconda3/envs/stats/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (3.0.4)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/models:\n",
    "!python -m spacy download es_core_news_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import es_core_news_md\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a generic function that cleans the documents in the `corpusList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article(article):\n",
    "    \"\"\"\n",
    "    TODO: write this.\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanText = article.cleanText\n",
    "    table = str.maketrans('', '', string.punctuation + \"¡¿\")\n",
    "    cleanText = cleanText.translate(table).lower()\n",
    "    \n",
    "    # Cleaning compound stopwords\n",
    "    for stopword in stopwords:\n",
    "        if len(stopword.split(\" \")) > 1:\n",
    "            cleanText = cleanText.replace(stopword, \"\")\n",
    "    \n",
    "    # Getting the bag of words representation\n",
    "    bagOfWords = []\n",
    "    for token in nlp(cleanText):\n",
    "        # Ignore stopwords\n",
    "        if token.text in stopwords:\n",
    "            continue\n",
    "\n",
    "        # Protect some words\n",
    "        if token.text in protectedWords:\n",
    "            # If the word is in the manual lemmas, we replace.\n",
    "            # Otherwise, we just add the word.\n",
    "            if token.text in manualLemmas.keys():\n",
    "                bagOfWords.append(manualLemmas[token.text])\n",
    "            else:\n",
    "                bagOfWords.append(token.text)\n",
    "\n",
    "        # For the rest, store lemmatas\n",
    "        else:\n",
    "            bagOfWords.append(token.lemma_)\n",
    "    \n",
    "    # Add the atribute to articles.\n",
    "    # TODO: some of the w here are spaces, I should\n",
    "    # remove them.\n",
    "    bagOfWords = [w for w in bagOfWords if w != \"\"]\n",
    "    bagOfWords = \" \".join(bagOfWords)\n",
    "    article.bagOfWords = bagOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I could run this in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in corpusList:\n",
    "    clean_article(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCorpus('../data/clean_json', corpusList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
