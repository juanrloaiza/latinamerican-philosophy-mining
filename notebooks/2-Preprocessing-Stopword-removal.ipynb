{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Preprocessing 2\n",
    "# Stopword and punctuation removal, lemmatization\n",
    "\n",
    "This notebook uses the lists of stopwords and protected words discussed in the previous notebook to clean the documents and arrive at a bag-of-words presentation of all documents. In this presentation, we stick with just the lemmata of all words that aren't stopwords.\n",
    "\n",
    "After running this notebook, all json documents will contain a new entry `bagOfWords` containing the clean representation, ready for `scikit-learn`s LDA. We will save these entries using our utility function defined in `util.loaders`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start this notebook by loading the corpus just as in the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus. Num. of articles: 771\n"
     ]
    }
   ],
   "source": [
    "from utils.corpus import Corpus\n",
    "\n",
    "corpus = Corpus(registry_path='utils/article_registry.json')\n",
    "corpusList = corpus.get_documents_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the wordlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the stopwords we defined, the protected words, and the dictionary of our manual lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"wordlists/stopwords.txt\") as fp:\n",
    "    stopwords = fp.read()\n",
    "\n",
    "with open(\"wordlists/protectedWords.txt\") as fp:\n",
    "    protected_words = fp.read()\n",
    "    \n",
    "with open(\"wordlists/manualLemmas.txt\") as fp:\n",
    "    manual_lemmas = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.split(\"\\n\"))\n",
    "protected_words = set(protected_words.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Spacy and its spanish model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using SpaCy, we will need to install their `es_core_news_md` Natural Language Processing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/models:\n",
    "# Run the next line to install the NLP model from SpaCy.\n",
    "# !python -m spacy download es_core_news_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 09:29:27.578596: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-04 09:29:27.578718: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import es_core_news_md\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a generic function that cleans the documents in the `corpusList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article(article):\n",
    "    \"\"\"\n",
    "    This function takes in an Article object (as constructed with the Article class (see utils)).\n",
    "    It processes the clean_text attribute of this object, which holds the text processed so far,\n",
    "    and implements the following steps:\n",
    "    \n",
    "        1. Short word removal: words that are less than 2 letters long are ignored.\n",
    "        2. Stopword removal: stopwords are also ignored.\n",
    "        3. Lemmatization: transforms each word into its corresponding lemma.\n",
    "        \n",
    "    It then saves the resulting bag of words into a bag_of_words attribute inside the Article object.\n",
    "    \n",
    "    Input: Article (object)\n",
    "    Return: None (function modifies object directly)\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    clean_text = article.clean_text\n",
    "    table = str.maketrans('', '', string.punctuation + \"¡¿\")\n",
    "    clean_text = clean_text.translate(table).lower()\n",
    "    \n",
    "    # Cleaning compound stopwords\n",
    "    for stopword in stopwords:\n",
    "        if len(stopword.split(\" \")) > 1:\n",
    "            clean_text = clean_text.replace(stopword, \"\")\n",
    "    \n",
    "    # Getting the bag of words representation\n",
    "    bag_of_words = []\n",
    "    for token in nlp(clean_text):\n",
    "        # Ignore short words and stopwords\n",
    "        if len(token.text) <= 2 or token.text in stopwords:\n",
    "            continue\n",
    "            \n",
    "            # NOTE: \"Yo\" might be an imporant word. Which other 2-letter words are important?\n",
    "            # NOTE 2: Eliminating 2-letter words also helps distinguish \"es\" from \"ser\".\n",
    "\n",
    "        # Protect some words\n",
    "        if token.text in protected_words:\n",
    "            \n",
    "            # If the word is in the manual lemmas, we replace.\n",
    "            # Otherwise, we just add the word.\n",
    "            if token.text in manual_lemmas.keys():\n",
    "                bag_of_words.append(manual_lemmas[token.text])\n",
    "            else:\n",
    "                bag_of_words.append(token.text)\n",
    "\n",
    "        # For the rest, store lemmatas\n",
    "        else:\n",
    "            bag_of_words.append(token.lemma_)\n",
    "    \n",
    "    # Add the atribute to articles.\n",
    "    bag_of_words = [w for w in bag_of_words if w != \"\"]\n",
    "    bag_of_words = \" \".join(bag_of_words)\n",
    "    \n",
    "    \n",
    "    # For some strange reason, there are weird blank characters in the bag of words,\n",
    "    # some of which are not regular spaces nor tabs nor line breaks. We implement an extra\n",
    "    # step that only includes word characters using regex before a final join.\n",
    "    bag_of_words = re.findall('\\w+', bag_of_words)\n",
    "    bag_of_words = \" \".join(bag_of_words)\n",
    "\n",
    "    \n",
    "    article.bag_of_words = bag_of_words\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the cleaning process in parallel using Python's `multiprocessing` library. By default we use 5 threads, but this can be changed according to the available number of threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: En efecto la implementación con Pool no comparte la lista global corpusList, lo que hace que no se guarden correctamente las ediciones sobre esta lista. El hotfix involucra una implementación en la cual la función `cleanArticle()` devuelve un objeto artículo que guardamos en una lista nueva, que es la que posteriormente guardamos a archivo. Sin embargo, esto duplica innecesariamente la corpusList en memoria. No es grave, pero una implementación correcta sería usando `Manager()` en lugar de `Pool`. Con `Manager()` sí podemos usar recursos compartidos y no necesitamos generar una lista nueva para los resultados procesados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 s, sys: 1.03 s, total: 3.07 s\n",
      "Wall time: 16min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "with Pool(5) as pool:\n",
    "    processedArticles = pool.map(clean_article, corpusList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the new corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the processed articles using the `save_documents()` method in the `Corpus` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.documents = processedArticles\n",
    "corpus.save_documents()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
