{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Preprocessing 2\n",
    "# Stopword and punctuation removal, lemmatization\n",
    "\n",
    "This notebook uses the lists of stopwords and protected words discussed in the previous notebook to clean the documents and arrive at a bag-of-words presentation of all documents. In this presentation, we stick with just the lemmata of all words that aren't stopwords.\n",
    "\n",
    "After running this notebook, all json documents will contain a new entry `bagOfWords` containing the clean representation, ready for `scikit-learn`s LDA. We will save these entries using our utility function defined in `util.loaders`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start this notebook by loading the corpus just as in the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys \n",
    "\n",
    "# Jupyter Notebooks are not good at handling relative imports.\n",
    "# Best solution (not great practice) is to add the project's path\n",
    "# to the module loading paths of sys.\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.loaders import loadCorpusList, saveCorpus\n",
    "\n",
    "corpusPath = '../data/corpus'\n",
    "\n",
    "corpusList = loadCorpusList(corpusPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusList = [doc for doc in corpusList if doc.lang == \"es\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1495"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpusList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the wordlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the stopwords we defined, the protected words, and the dictionary of our manual lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wordlists/stopwords.txt\") as fp:\n",
    "    stopwords = fp.read()\n",
    "\n",
    "with open(\"wordlists/protectedWords.txt\") as fp:\n",
    "    protectedWords = fp.read()\n",
    "    \n",
    "with open(\"wordlists/manualLemmas.txt\") as fp:\n",
    "    manualLemmas = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.split(\"\\n\"))\n",
    "protectedWords = set(protectedWords.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Spacy and its spanish model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using SpaCy, we will need to install their `es_core_news_md` Natural Language Processing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/models:\n",
    "# Run the next line to install the NLP model from SpaCy.\n",
    "# !python -m spacy download es_core_news_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import es_core_news_md\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a generic function that cleans the documents in the `corpusList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanArticle(article):\n",
    "    \"\"\"\n",
    "    This function takes in an Article object (as constructed with the Article class (see utils)).\n",
    "    It processes the cleanText attribute of this object, which holds the text processed so far,\n",
    "    and implements the following steps:\n",
    "    \n",
    "        1. Short word removal: words that are less than 2 letters long are ignored.\n",
    "        2. Stopword removal: stopwords are also ignored.\n",
    "        3. Lemmatization: transforms each word into its corresponding lemma.\n",
    "        \n",
    "    It then saves the resulting bag of words into a bagOfWords attribute inside the Article object.\n",
    "    \n",
    "    Input: Article (object)\n",
    "    Return: None (function modifies object directly)\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanText = article.cleanText\n",
    "    table = str.maketrans('', '', string.punctuation + \"¡¿\")\n",
    "    cleanText = cleanText.translate(table).lower()\n",
    "    \n",
    "    # Cleaning compound stopwords\n",
    "    for stopword in stopwords:\n",
    "        if len(stopword.split(\" \")) > 1:\n",
    "            cleanText = cleanText.replace(stopword, \"\")\n",
    "    \n",
    "    # Getting the bag of words representation\n",
    "    bagOfWords = []\n",
    "    for token in nlp(cleanText):\n",
    "        # Ignore short words and stopwords\n",
    "        if len(token.text) <= 2 or token.text in stopwords:\n",
    "            continue\n",
    "            \n",
    "            # NOTE: \"Yo\" might be an imporant word. Which other 2-letter words are important?\n",
    "            # NOTE 2: Eliminating 2-letter words also helps distinguish \"es\" from \"ser\".\n",
    "\n",
    "        # Protect some words\n",
    "        if token.text in protectedWords:\n",
    "            # If the word is in the manual lemmas, we replace.\n",
    "            # Otherwise, we just add the word.\n",
    "            if token.text in manualLemmas.keys():\n",
    "                bagOfWords.append(manualLemmas[token.text])\n",
    "            else:\n",
    "                bagOfWords.append(token.text)\n",
    "\n",
    "        # For the rest, store lemmatas\n",
    "        else:\n",
    "            bagOfWords.append(token.lemma_)\n",
    "    \n",
    "    # Add the atribute to articles.\n",
    "    # TODO: some of the w here are spaces, I should\n",
    "    # remove them.\n",
    "    bagOfWords = [w for w in bagOfWords if w != \"\"]\n",
    "    bagOfWords = \" \".join(bagOfWords)\n",
    "    article.bagOfWords = bagOfWords\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the cleaning process in parallel using Python's `multiprocessing` library. By default we use 5 threads, but this can be changed according to the available number of threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: En efecto la implementación con Pool no comparte la lista global corpusList, lo que hace que no se guarden correctamente las ediciones sobre esta lista. El hotfix involucra una implementación en la cual la función `cleanArticle()` devuelve un objeto artículo que guardamos en una lista nueva, que es la que posteriormente guardamos a archivo. Sin embargo, esto duplica innecesariamente la corpusList en memoria. No es grave, pero una implementación correcta sería usando `Manager()` en lugar de `Pool`. Con `Manager()` sí podemos usar recursos compartidos y no necesitamos generar una lista nueva para los resultados procesados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 740.46s\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "with Pool(6) as pool:\n",
    "    processedArticles = pool.map(cleanArticle, corpusList)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time elapsed: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the new corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the processed articles using our `saveCorpus` utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCorpus('../data/corpus', processedArticles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
